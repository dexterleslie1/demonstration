version: "3.8"

services:
#  service:
#    build:
#      context: ./service
#      dockerfile: Dockerfile
#    image: registry.cn-hangzhou.aliyuncs.com/future-public/demo-kafka-benchmark-service
#    environment:
#      - JAVA_OPTS=-Xmx1g
#      - TZ=Asia/Shanghai
#      - kafka_bootstrap_servers=kafka
#    ports:
#      - '8080:8080'
#  crond:
#    build:
#      context: ./crond
#      dockerfile: Dockerfile
#    image: registry.cn-hangzhou.aliyuncs.com/future-public/demo-kafka-benchmark-crond
#    environment:
#      - JAVA_OPTS=-Xmx1g
#      - TZ=Asia/Shanghai
#      - kafka_bootstrap_servers=kafka

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      TZ: Asia/Shanghai
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # todo 设置 ZooKeeper 的 JVM 堆内存
      # ZOOKEEPER_HEAP_OPTS: "-Xms1g -Xmx1g"
    ports:
      - "2181:2181"
  kafka1:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      # 映射 JMX 端口到主机
      # - "9997:9997"
    environment:
      TZ: Asia/Shanghai
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${kafka_advertised_listeners}:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # 设置 Kafka 的 JVM 堆内存
      KAFKA_HEAP_OPTS: "-Xms1g -Xmx1g"
      # 配置 JMX 端口和认证，配置了 jmx 端口才能够被外部工具监控
      # KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.port=9997 -Dcom.sun.management.jmxremote.rmi.port=9997"
      # 禁用自动创建 Topic，否则 Spring Boot 会自动创建 partitions=0 的 topic
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      # ------------------- 事务配置 -------------------
      # 需要下面配置，否则 Kafka 报告下面错误
      # Error processing create topic request CreatableTopic(name='__transaction_state', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='uncompressed'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2'), CreateableTopicConfig(name='segment.bytes', value='104857600'), CreateableTopicConfig(name='unclean.leader.election.enable', value='false')]) (kafka.server.ZkAdminManager)
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      # ------------------- 日志清理配置 -------------------
      # 清理策略：启用 delete（按时间/大小删除）
      KAFKA_LOG_CLEANUP_POLICY: "delete"
      # 单个分区最大日志大小：2G（根据磁盘容量调整，如 5GB、20GB）
      KAFKA_LOG_RETENTION_BYTES: "2147483648"  # 设为 -1 表示不限制大小（仅用时间策略）
      # 日志段大小：512MB（更小的段可提升清理精度，但增加文件数）
      KAFKA_LOG_SEGMENT_BYTES: "536870912"  #（原默认 1GB）
#      # 单个分区最大日志大小：1MB（根据磁盘容量调整，如 5GB、20GB）
#      KAFKA_LOG_RETENTION_BYTES: "1048576"  # 设为 -1 表示不限制大小（仅用时间策略）
#      # 日志段大小：256KB（更小的段可提升清理精度，但增加文件数）
#      KAFKA_LOG_SEGMENT_BYTES: "262144"  #（原默认 1GB）
#  kafka2:
#    image: confluentinc/cp-kafka:7.3.0
#    depends_on:
#      - zookeeper
#    ports:
#      - "9093:9093"
#      # 映射 JMX 端口到主机
#      # - "9997:9997"
#    environment:
#      TZ: Asia/Shanghai
#      KAFKA_BROKER_ID: 2
#      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${kafka_advertised_listeners}:9093
#      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#      # 设置 Kafka 的 JVM 堆内存
#      KAFKA_HEAP_OPTS: "-Xms1g -Xmx1g"
#      # 配置 JMX 端口和认证，配置了 jmx 端口才能够被外部工具监控
#      # KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.port=9997 -Dcom.sun.management.jmxremote.rmi.port=9997"
#      # 禁用自动创建 Topic，否则 Spring Boot 会自动创建 partitions=0 的 topic
#      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
#      # ------------------- 事务配置 -------------------
#      # 需要下面配置，否则 Kafka 报告下面错误
#      # Error processing create topic request CreatableTopic(name='__transaction_state', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='uncompressed'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2'), CreateableTopicConfig(name='segment.bytes', value='104857600'), CreateableTopicConfig(name='unclean.leader.election.enable', value='false')]) (kafka.server.ZkAdminManager)
#      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
#      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
#      # ------------------- 日志清理配置 -------------------
#      # 清理策略：启用 delete（按时间/大小删除）
#      KAFKA_LOG_CLEANUP_POLICY: "delete"
#      # 单个分区最大日志大小：2G（根据磁盘容量调整，如 5GB、20GB）
#      KAFKA_LOG_RETENTION_BYTES: "2147483648"  # 设为 -1 表示不限制大小（仅用时间策略）
#      # 日志段大小：512MB（更小的段可提升清理精度，但增加文件数）
#      KAFKA_LOG_SEGMENT_BYTES: "536870912"  #（原默认 1GB）
#  #      # 单个分区最大日志大小：1MB（根据磁盘容量调整，如 5GB、20GB）
#  #      KAFKA_LOG_RETENTION_BYTES: "1048576"  # 设为 -1 表示不限制大小（仅用时间策略）
#  #      # 日志段大小：256KB（更小的段可提升清理精度，但增加文件数）
#  #      KAFKA_LOG_SEGMENT_BYTES: "262144"  #（原默认 1GB）
  # Kafka Prometheus Exporter
  kafka-exporter:
    image: danielqsj/kafka-exporter:v1.9.0 # 使用官方镜像
    ports:
      - "9308:9308" # 将宿主机的9308端口映射到容器端口，用于Prometheus抓取和手动访问
    command: # 最重要的部分：启动命令和参数
      - --kafka.server=kafka1:9092 # 指向Kafka Broker地址
      # 如果你的Kafka有多个Broker，可以添加多个--kafka.server参数
      # - --kafka.server=kafka-broker-2:9092
      # - --kafka.server=kafka-broker-3:9092
      # 其他常用可选参数：
      - --web.telemetry-path=/metrics # 指标暴露的路径，默认为/metrics，通常无需修改
      - --log.level=info # 日志级别：debug, info, warn, error
      # - --sasl.enabled # 如果Kafka需要SASL认证，请取消注释并配置以下参数
      # - --sasl.username=your_username
      # - --sasl.password=your_password
      # - --sasl.mechanism=PLAIN
      # - --tls.enabled # 如果Kafka需要SSL/TLS，请取消注释
      # - --tls.ca-file=/path/to/ca.pem # 需要挂载证书文件
      # - --tls.cert-file=/path/to/service.cert
      # - --tls.key-file=/path/to/service.key
    depends_on:
      - kafka1

  # 在 kafka 服务成功启动后自动配置 kafka 服务
  kafka-create-topic1:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - kafka1
    environment:
      TZ: Asia/Shanghai
    # 自动创建 topic
    entrypoint: /usr/bin/kafka-topics --create --bootstrap-server kafka1:9092 --replication-factor 1 --partitions 256 --topic my-topic-1
  kafka-create-topic2:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - kafka1
    environment:
      TZ: Asia/Shanghai
    # 自动创建 topic
    entrypoint: /usr/bin/kafka-topics --create --bootstrap-server kafka1:9092 --replication-factor 1 --partitions 2 --topic my-topic-2
  kafka-create-topic-test-send-perf:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - kafka1
    environment:
      TZ: Asia/Shanghai
    # 自动创建 topic
    entrypoint: /usr/bin/kafka-topics --create --bootstrap-server kafka1:9092 --replication-factor 1 --partitions 1 --topic topic-test-send-perf
  kafka-create-topic-test-alter-partitions-online:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - kafka1
    environment:
      TZ: Asia/Shanghai
    entrypoint: /usr/bin/kafka-topics --create --bootstrap-server kafka1:9092 --replication-factor 1 --partitions 1 --topic topic-test-alter-partitions-online
  kafka-create-topic-test-assist-transaction-topic-1:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - kafka1
    environment:
      TZ: Asia/Shanghai
    entrypoint: /usr/bin/kafka-topics --create --bootstrap-server kafka1:9092 --replication-factor 1 --partitions 256 --topic test-assist-transaction-topic-1
  kafka-create-topic-test-assist-transaction-topic-2:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - kafka1
    environment:
      TZ: Asia/Shanghai
    entrypoint: /usr/bin/kafka-topics --create --bootstrap-server kafka1:9092 --replication-factor 1 --partitions 256 --topic test-assist-transaction-topic-2
  kafka-create-topic-test-assist-transaction-topic-3:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - kafka1
    environment:
      TZ: Asia/Shanghai
    entrypoint: /usr/bin/kafka-topics --create --bootstrap-server kafka1:9092 --replication-factor 1 --partitions 256 --topic test-assist-transaction-topic-3

  kafka-manager:
    image: sheepkiller/kafka-manager:latest
    ports:
      - "9000:9000"
    environment:
      TZ: Asia/Shanghai
      ZK_HOSTS: zookeeper:2181
      APPLICATION_SECRET: "123456"

  redis:
    image: redis:7.2.4
    # 设置redis密码
    command: redis-server /usr/local/etc/redis/redis.conf --requirepass 123456
    volumes:
      - ./deployer/common/redis7.conf:/usr/local/etc/redis/redis.conf
    environment:
      - TZ=Asia/Shanghai
    network_mode: host
