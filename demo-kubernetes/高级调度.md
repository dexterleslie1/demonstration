# 高级调度

## 使用污点和容忍度阻止节点调度到特定节点

>注意：节点删除污点后，不管哪个污点效果系统都不会自动重新调度`pod`到当前被移除污点后的节点。

节点选择器和节点亲缘性规则，是通过明确的在`pod`中添加的信息，来决定一个`pod`可以或者不可以被调度到那些节点上。而污点则是在不修改已有的`pod`信息的前提下，通过在节点上添加污点信息，来拒绝`pod`在某系节点上的部署

**污点效果**

- `NoSchedule`表示如果`pod`没有容忍这些污点，`pod`则不能被调度到包含这些污点的污点上。对目前已经在运行并且没有容忍这个污点的`pod`不会产生影响。
- `PreferNoSchedule`是`NoSchedule`的一个宽松的版本，表示尽量阻止`pod`被调度到这个节点上，但是如果没有其他节点可以调度，`pod`依然会被调度到这个节点上。
- `NoExecute`不同于`NoSchedule`以及`PreferNoSchedule`，后两者只在调度期间起作用，而`NoExecute`也会影响正在节点上运行着的`pod`。如果在一个节点上添加了`NoExecute`污点，那些在该节点上运行着的`pod`，如果没有容忍这个`NoExecute`污点，将会从这个节点去除。



### 主节点的污点和系统级别`pod`的污点容忍度

```shell
# 默认情况下，集群中的主节点需要设置污点，这样才能保证只有控制面板pod才能够部署在主节点上
# 可以看到主节点被添加污点(taints) node-role.kubernetes.io/master:NoSchedule
kubectl describe node demo-k8s-master

# 因为kube-proxy pod添加了污点容忍度node-role.kubernetes.io/master:NoSchedule
# todo 为何kubernetes1.23.0的kube-proxy-xxx pod没有ode-role.kubernetes.io/master:NoSchedule容忍能够调度到master节点上呢？
# 所以kube-proxy能够被调度到master节点上
kubectl get pod -n kube-system -o wide | grep kube-proxy
kubectl describe pod kube-proxy-825tq -n kube-system
```



### `NoSchedule`污点效果

```shell
# 给节点demo-k8s-node1添加污点node-type=production:NoSchedule
kubectl taint node demo-k8s-node1 node-type=production:NoSchedule

# 查看节点污点信息
kubectl describe node demo-k8s-node1

# 部署没有容忍度的pod
apiVersion: apps/v1
kind: Deployment
metadata:
 name: test
spec:
 replicas: 9
 selector:
  matchLabels:
   app: test
 template:
  metadata:
   labels:
    app: test
  spec:
   containers:
   - name: nginx
     image: nginx
     imagePullPolicy: IfNotPresent
    
    
# 查看pod没有被调度到节点demo-k8s-node1上
kubectl get pod -o wide

# 删除节点污点node-type=production:NoSchedule
kubectl taint node demo-k8s-node1 node-type=production:NoSchedule-
```

### `NoExecute`污点效果

```shell
# 用于创建pod 
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment1
spec:
 replicas: 9
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: kubia
      image: busybox
      imagePullPolicy: IfNotPresent
      command: ["sh", "-c", "sleep 7200;"]

# 创建pod
kubectl apply -f 1.yaml 

# 查看所有pod
kubectl get pod -o wide

# 标记demo-k8s-node1 NoExecute污点
kubectl taint node demo-k8s-node1 node-type=production:NoExecute

# 在节点demo-k8s-node1上的pod被重新调度到别的节点
kubectl get pod -o wide

# 删除污点
kubectl taint node demo-k8s-node1 node-type=production:NoExecute-
```



### 创建`pod`时添加污点容忍度

```shell
# 标记demo-k8s-node1 NoSchedule污点
kubectl taint node demo-k8s-node1 node-type=production:NoSchedule

# 创建带容忍度的pod
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment1
spec:
 replicas: 9
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   tolerations:
   - key: node-type
     operator: Equal
     value: production
     effect: NoSchedule
   containers:
    - name: kubia
      image: busybox
      imagePullPolicy: IfNotPresent
      command: ["sh", "-c", "sleep 7200;"]

# 查看pod也会被调度到节点demo-k8s-node1
kubectl get pod -o wide

# 删除污点
kubectl taint node demo-k8s-node1 node-type=production:NoSchedule-
```



### 容忍节点上的所有效果为`NoSchedule`的污点

```yaml
tolerations:
- operator: Exists
  effect: NoSchedule
```



## 定向调度

> 通过nodename或者nodeselector指定pod运行的节点

### 使用nodename指定节点

```shell
# 用于创建pod
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: nginx
      image: nginx
   nodeName: demo-k8s-node1 # 使用节点名称指定pod运行节点
  
# 所有节点被调度到demo-k8s-node1上
kubectl get pod -o wide
```



### 使用`nodeselector`指定节点标签

```shell
# 节点打标签
kubectl label node demo-k8s-node1 node-label=node1

# 用于创建pod 
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: nginx
      image: nginx
   nodeSelector:
    node-label: node1 # 指定节点标签

# 所有pod被schedule到demo-k8s-node1
kubectl get pod -o wide

# 删除节点标签
kubectl label node demo-k8s-node1 node-label-
```



### 强制调度到`k8s master`节点上

>[Kubernetes: Make Pods Run on Your Master Nodes](https://ekartco.com/2017/09/kubernetes-make-pods-run-on-your-master-nodes/)

```yaml
# 选择k8s master节点标签
nodeSelector:
	node-role.kubernetes.io/master: ""
# 需要容忍k8s master节点上的污点，否则无法调度
tolerations:
- effect: NoSchedule
key: node-role.kubernetes.io/master
```



## 使用节点亲缘性将pod调度到特定节点上

### 节点亲缘性(node affinity)

> 硬限制 requiredDuringSchedulingIgnoredDuringExecution，不匹配不调度，pod一直处于pending状态。
>
> 软限制 preferredDuringSchedulingIgnoredDuringExecution，不匹配也调度。

情景1（无法匹配节点，pod无法调度，一直处于pending状态）

```shell
# 因为没有节点labels包含nodeenv in xxx,yyy，所以deployment无法调度一直pending状态 
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
   affinity:
    nodeAffinity:
     # 硬限制
     requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
       # 匹配节点labels in xxx,yyy
       - matchExpressions:
          - key: nodeenv
            operator: In
            values:
             - xxx
             - yyy
             
# 不匹配不调度，一直处于pending状态
kubectl get pod -o wide
```

情景2（给demo-k8s-node2节点打上标签，成功匹配，所有pod被调度到此节点）

```shell
# 给节点demo-k8s-node2打上标签成功使用nodeAffinity调度pod
kubectl label node demo-k8s-node2 nodeenv=prod

# 用于创建pod
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
   affinity:
    nodeAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
       - matchExpressions:
          - key: nodeenv
            operator: In
            values:
             - prod
             - yyy
             
# 所有pod被成功调度到demo-k8s-node2
kubectl get pod -o wide

# 删除节点标签
kubectl label node demo-k8s-node2 nodeenv-
```

情景3（软限制，即使不匹配节点，最终所有pod被随机调度到各个节点）

```shell
# 用于创建pod
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
   affinity:
    nodeAffinity:
     preferredDuringSchedulingIgnoredDuringExecution:
      # 1-100权重参数，pod 优先调度到权重总和最高的节点上
      - weight: 1
        preference: 
         matchExpressions:
          - key: nodeenv
            operator: In
            values:
             - xxx
             - yyy
# 创建pod
kubectl apply -f 1.yaml

# 即使不匹配，pod也会被调度到各个节点
kubectl get pod -o wide
```

情景4（软限制，有匹配的节点标签，所有pod被调度到此节点）

```shell
# 节点打标签
kubectl label node demo-k8s-node2 nodeenv=prod

# 用于创建pod
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
   affinity:
    nodeAffinity:
     preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference: 
         matchExpressions:
          - key: nodeenv
            operator: In
            values:
             - prod
             - yyy
             
# 节点有标签对应，所有pod被调度到相应的节点
kubectl get pod -o wide

# 删除标签
kubectl label node demo-k8s-node2 nodeenv-
```



### pod亲缘性(pod affinity)

**硬限制不能匹配pod一直pending状态情景**

```shell
# 创建参考点pod，指定参考点pod被调度到demo-k8s-node2上
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment1
spec:
 replicas: 2
 selector:
  matchLabels:
   podenv: dev
 template:
  metadata:
   labels:
    podenv: dev
  spec:
   containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
   nodeName: demo-k8s-node2

---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment2
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: busybox
      image: busybox
      imagePullPolicy: IfNotPresent
   affinity:
    podAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
         matchExpressions:
          - key: podenv
            operator: In
            values:
             - prod
             - yyy
        topologyKey: kubernetes.io/hostname

# 创建不存在的pod label导致无法匹配pod一直处于pending状态
kubectl get pod -o wide
```

**硬限制能够匹配pod并且成功调度**

```shell
# 创建参考pod
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment1
spec:
 replicas: 2
 selector:
  matchLabels:
   podenv: dev
 template:
  metadata:
   labels:
    podenv: dev
  spec:
   containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
   nodeName: demo-k8s-node2

---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment2
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: busybox
      image: busybox
      imagePullPolicy: IfNotPresent
      command: ["sh", "-c", "sleep 7200;"]
   affinity:
    podAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
         matchExpressions:
          - key: podenv
            operator: In
            values:
             - dev
             - yyy
        topologyKey: kubernetes.io/hostname
        
# pod被成功调度到demo-k8s-node2上
kubectl get pod -o wide
```

**pod非亲缘性(pod anti affinity)**

情景1，硬限制

```shell
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment1
spec:
 replicas: 2
 selector:
  matchLabels:
   podenv: dev
 template:
  metadata:
   labels:
    podenv: dev
  spec:
   containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
   nodeName: demo-k8s-node2

---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment2
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: busybox
      image: busybox
      imagePullPolicy: IfNotPresent
      command: ["sh", "-c", "sleep 7200;"]
   affinity:
    podAntiAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
         matchExpressions:
          - key: podenv
            operator: In
            values:
             - dev
             - yyy
        topologyKey: kubernetes.io/hostname

# deployment2没有pod被调度到demo-k8s-node2节点上
kubectl get pod -o wide
```

情景2，软限制

```sh
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment1
spec:
 replicas: 1
 selector:
  matchLabels:
   podenv: dev
 template:
  metadata:
   labels:
    podenv: dev
  spec:
   containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
   nodeName: demo-k8s-node2

---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: deployment2
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubia
 template:
  metadata:
   labels:
    app: kubia
  spec:
   containers:
    - name: busybox
      image: busybox
      imagePullPolicy: IfNotPresent
      command: ["sh", "-c", "sleep 7200;"]
   affinity:
    podAntiAffinity:
     # 表示尽量不和有标签 podenv=dev 的pod在同一个节点上
     preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: podenv
                operator: In
                values:
                - dev
                - yyy
          topologyKey: kubernetes.io/hostname
          
# 创建 pod
kubectl apply -f 1.yal

# deployment2 和 deployment1 中的 pod 不会在同一个节点上
kubectl get pod -o wide
```



## 平均地调度pod到所有节点中

> https://cloudhero.io/kubernetes-evenly-distribution-of-pods-across-cluster-nodes/

```shell
# 8个pod会被平均地分配到4个节点中
apiVersion: apps/v1
kind: Deployment
metadata:
 name: test
spec:
 replicas: 8
 selector:
  matchLabels:
   app: test
 template:
  metadata:
   labels:
    app: test
  spec:
   # 平均地调度pod到所有节点中
   topologySpreadConstraints:
   - maxSkew: 1
     topologyKey: kubernetes.io/hostname
     whenUnsatisfiable: ScheduleAnyway
     labelSelector:
      matchLabels:
       app: test
   containers:
   - name: test
     image: busybox
     command: ["sleep", "3600"]
     imagePullPolicy: IfNotPresent
```

